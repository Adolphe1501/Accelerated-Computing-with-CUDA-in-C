<h1 id="acceleratedcomputingwithcudainc">Accelerated Computing with CUDA in C</h1>

<p>In this we will be looking at some basic commands for programming in CUDA for creating acceleratied appliccation. So lets see what is cuda.
 CUDA is a parallel computing platform and programming model that enables dramatic increases in computing performance by harnessing the power of the GPU. </p>

<p>(The commands listed here are for the jupyter notebook environment)</p>

<blockquote>
  <p>For retrieving the NVIDIA GPU info - <code>!nvidia-smi</code></p>
  
  <h3 id="pointstonotewhenwrittingagpucode">Points to note when writting a GPU code</h3>
  
  <ul>
  <li>The gpu accelerated fucntons are saved with an extension > "<strong>.cu</strong>"</li>
  
  <li>The gpu functions should be  always declared as a global function <code>__global__ void GPUFunction</code><br />One thing to note here is the return type. The return type should be -void- since it is declared as global</li>
  
  <li>During the function call it should follow the syntax <code>GPUFunction&lt;&lt;&lt;1, 1&gt;&gt;&gt;();</code> <br /> We will be looking into this syntax in detail.But, basic idea is when we are calling a gpu function (which we call as kernel) <code>&lt;&lt;&lt; &gt;&gt;&gt;</code> provides an execution config for the GPU.</li>
  
  <li>When you are executing a GPU function, it happens asynchronously with the CPU process. So we need to have something which will ensure the synchronization between CPU and GPU process. <code>cudaDeviceSynchronize();</code> will ensure this. Add this immidiately after you call a GPU function.</li>
  
  <li>Putting all together, we can write a gpu code as given below </li>
  </ul>
</blockquote>

<pre><code class="  language- ">#include &lt;stdio.h&gt;
__global__ void helloGPU()
{
  printf("Hello from the GPU.\n");
}

int main()
{

  helloGPU&lt;&lt;&lt;1, 1&gt;&gt;&gt;();

  cudaDeviceSynchronize();
} 
</code></pre>

<h3 id="launchingparallelkernels">Launching Parallel Kernels</h3>

<ul>
<li>Earlier we saw how to call a GPU fucntion . <code>helloGPU&lt;&lt;&lt;1, 1&gt;&gt;&gt;();</code>. So what does &lt;&lt;&lt; >>> actually do? The execution configuration allows programmers to specifiy how many groups of threads - called thread blocks, or just blocks - and how many threads they would like each thread block to contain.</li>

<li><code>&lt;&lt;&lt; NUMBER_OF_BLOCKS, NUMBER_OF_THREADS_PER_BLOCK&gt;&gt;&gt;</code> where BLOCKS is a collection of threads and each blocks contain equal amount of threads.</li>
</ul>

<h4 id="acceleratingforloops">Accelerating for loops</h4>

<ul>
<li>The following codes will show the effectiveness of using GPU instead of CPU for a for loop.</li>
</ul>

<pre><code>#include &lt;stdio.h&gt;

void CPUloop(int N)
{
  for (int i = 0; i &lt; N; ++i)
  {
    printf("This is iteration number %d\n", i);
  }
}

int main()
{

  int N = 10;
  loop(N);
}
</code></pre>

<p>Now lets see how we can use GPUs to accelerate this for loop</p>

<pre><code>#include &lt;stdio.h&gt;

__global__ void loop()
{

  printf("This is iteration number %d\n", threadIdx.x);
}

int main()
{

  loop&lt;&lt;&lt;1, 10&gt;&gt;&gt;();
  cudaDeviceSynchronize();
}
</code></pre>

<ul>
<li>Notice the <code>threadIdx.x</code> which was used in the above code</li>

<li>CUDA kernels have access to special variables identifying both the index of the thread (within the block) that is executing the kernel, and, the index of the block (within the grid) that the thread is within. These variables are <strong>threadIdx.x</strong> and <strong>blockIdx.x</strong> respectively.</li>
</ul>

<h3 id="moreparallelization">More Parallelization</h3>

<ul>
<li>The maximum number of blocks that can be inside a block is 1024. In order to increase the amount of parallelism in accelerated applications, we must be able to coordinate among multiple thread blocks.</li>

<li><code>blockdim.x</code> says how many threads are there in a block. With this we can find the unique indexing for the threads &lt; br/> <code>threadIdx.x + blockIdx.x * blockDim.x</code></li>

<li>Lets see an example code for more clarity</li>
</ul>

<pre><code>#include &lt;stdio.h&gt;

__global__ void loop()
{

  int i = blockIdx.x * blockDim.x + threadIdx.x;
  printf("%d\n", i);
}

int main()
{

  loop&lt;&lt;&lt;2, 5&gt;&gt;&gt;();
  cudaDeviceSynchronize();
}
</code></pre>

<h3 id="dynamicmemoryallocation">Dynamic memory allocation</h3>

<p>We can allocate memory such that it can be accessed from GPU as well as CPU by which the data tranfer process will occur much faster</p>

<ul>
<li>Dynamic memory allocation in the CPU <code>a = (int *)malloc(size);</code></li>
</ul>

<h2 id="sharedmemoryallocationcudamallocmanagedasize">- Shared memory allocation <code>cudaMallocManaged(&amp;a, size);</code></h2>

<h3 id="datasetslargerthanthegrid">Data Sets Larger Than the Grid</h3>

<ul>
<li>These days it is so often to see the datasets which has more than the total number of threads . So we nned something that can make a loop and reuse the threads when current procrss is over. This is known a <strong>Grid Stride Loop</strong></li>

<li>Lets see an example code which is used to doubles element values in an arrya using Grid Stride Loop </li>
</ul>

<pre><code>#include &lt;stdio.h&gt;

void init(int *a, int N)
{
  int i;
  for (i = 0; i &lt; N; ++i)
  {
    a[i] = i;
  }
}

__global__
void doubleElements(int *a, int N)
{

  int idx = blockIdx.x * blockDim.x + threadIdx.x;
  int stride = gridDim.x * blockDim.x;

  for (int i = idx; i &lt; N; i += stride)
  {
    a[i] *= 2;
  }
}


int main()
{
  int N = 10000;
  int *a;

  size_t size = N * sizeof(int);
  cudaMallocManaged(&amp;a, size);  // Allocationg shared memory

  init(a, N);

  size_t threads_per_block = 256;
  size_t number_of_blocks = 32;

  doubleElements&lt;&lt;&lt;number_of_blocks, threads_per_block&gt;&gt;&gt;(a, N);
  cudaDeviceSynchronize();

  bool areDoubled = checkElementsAreDoubled(a, N);

  cudaFree(a);  //To free the allocated memory
}
</code></pre>
